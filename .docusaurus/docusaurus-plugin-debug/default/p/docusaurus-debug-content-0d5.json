{"allContent":{"docusaurus-plugin-content-docs":{"default":{"loadedVersions":[{"versionName":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","path":"/docs","tagsPath":"/docs/tags","editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs","isLast":true,"routePriority":-1,"sidebarFilePath":"E:\\GIAIC\\Quarter 4\\Hackathon\\hackathon\\website\\sidebars.js","contentPath":"E:\\GIAIC\\Quarter 4\\Hackathon\\hackathon\\website\\docs","docs":[{"id":"AUTHENTICATION_GUIDE","title":"Authentication & Personalization Implementation Guide","description":"Overview","source":"@site/docs/AUTHENTICATION_GUIDE.md","sourceDirName":".","slug":"/AUTHENTICATION_GUIDE","permalink":"/docs/AUTHENTICATION_GUIDE","draft":false,"unlisted":false,"editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs/AUTHENTICATION_GUIDE.md","tags":[],"version":"current","frontMatter":{}},{"id":"chapters/ch0-preface","title":"Preface","description":"Welcome to Physical AI & Humanoid Robotics Essentials. This textbook teaches you how to build AI-powered humanoid robots from first principles.","source":"@site/docs/chapters/ch0-preface.mdx","sourceDirName":"chapters","slug":"/chapters/ch0-preface","permalink":"/docs/chapters/ch0-preface","draft":false,"unlisted":false,"editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs/chapters/ch0-preface.mdx","tags":[],"version":"current","frontMatter":{"id":"ch0-preface","title":"Preface","sidebar_label":"Preface","description":"Welcome to Physical AI & Humanoid Robotics Essentials. This textbook teaches you how to build AI-powered humanoid robots from first principles."},"sidebar":"tutorialSidebar","next":{"title":"Prerequisites","permalink":"/docs/chapters/ch0-prerequisites"}},{"id":"chapters/ch0-prerequisites","title":"Prerequisites & Notation","description":"Mathematical notation, assumed knowledge, and quick refresher on linear algebra and physics concepts used throughout the textbook.","source":"@site/docs/chapters/ch0-prerequisites.mdx","sourceDirName":"chapters","slug":"/chapters/ch0-prerequisites","permalink":"/docs/chapters/ch0-prerequisites","draft":false,"unlisted":false,"editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs/chapters/ch0-prerequisites.mdx","tags":[],"version":"current","frontMatter":{"id":"ch0-prerequisites","title":"Prerequisites & Notation","sidebar_label":"Prerequisites","description":"Mathematical notation, assumed knowledge, and quick refresher on linear algebra and physics concepts used throughout the textbook."},"sidebar":"tutorialSidebar","previous":{"title":"Preface","permalink":"/docs/chapters/ch0-preface"},"next":{"title":"Ch1: Intro to Physical AI","permalink":"/docs/chapters/ch1-intro"}},{"id":"chapters/ch1-intro","title":"Chapter 1: Introduction to Physical AI","description":"Define Physical AI as embodied AI through perception-action loops. Explore why humanoid robots matter and the convergence of AI + embodied systems in 2025.","source":"@site/docs/chapters/ch1-intro.mdx","sourceDirName":"chapters","slug":"/chapters/ch1-intro","permalink":"/docs/chapters/ch1-intro","draft":false,"unlisted":false,"editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs/chapters/ch1-intro.mdx","tags":[],"version":"current","frontMatter":{"id":"ch1-intro","title":"Chapter 1: Introduction to Physical AI","sidebar_label":"Ch1: Intro to Physical AI","description":"Define Physical AI as embodied AI through perception-action loops. Explore why humanoid robots matter and the convergence of AI + embodied systems in 2025.","keywords":["physical ai","embodiment","robotics","humanoids","tesla optimus","figure ai","boston dynamics","closed-loop","perception"]},"sidebar":"tutorialSidebar","previous":{"title":"Prerequisites","permalink":"/docs/chapters/ch0-prerequisites"},"next":{"title":"M1: Overview","permalink":"/docs/modules/m1-ros2/m1-overview"}},{"id":"chapters/ch2-humanoid","title":"Chapter 2: Basics of Humanoid Robotics","description":"Kinematics, dynamics, actuators, and sensing—the engineering fundamentals of humanoid robots. Practical calculations for reaching, grasping, and movement.","source":"@site/docs/chapters/ch2-humanoid.mdx","sourceDirName":"chapters","slug":"/chapters/ch2-humanoid","permalink":"/docs/chapters/ch2-humanoid","draft":false,"unlisted":false,"editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs/chapters/ch2-humanoid.mdx","tags":[],"version":"current","frontMatter":{"id":"ch2-humanoid","title":"Chapter 2: Basics of Humanoid Robotics","sidebar_label":"Ch2: Humanoid Basics","description":"Kinematics, dynamics, actuators, and sensing—the engineering fundamentals of humanoid robots. Practical calculations for reaching, grasping, and movement.","keywords":["kinematics","dynamics","actuators","dof","energy efficiency","forward kinematics","inverse kinematics","sensors","imu","force sensors"]}},{"id":"chapters/ch3-ros2","title":"Chapter 3: ROS 2 Fundamentals","description":"Nodes, topics, services, packages—building modular robot software with ROS 2 Humble. Practical workflows for real robot systems.","source":"@site/docs/chapters/ch3-ros2.mdx","sourceDirName":"chapters","slug":"/chapters/ch3-ros2","permalink":"/docs/chapters/ch3-ros2","draft":false,"unlisted":false,"editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs/chapters/ch3-ros2.mdx","tags":[],"version":"current","frontMatter":{"id":"ch3-ros2","title":"Chapter 3: ROS 2 Fundamentals","sidebar_label":"Ch3: ROS 2 Fundamentals","description":"Nodes, topics, services, packages—building modular robot software with ROS 2 Humble. Practical workflows for real robot systems.","keywords":["ros2","nodes","topics","services","packages","colcon","middleware","dds","python","rclpy"]}},{"id":"chapters/ch4-sim","title":"Chapter 4: Digital Twin Simulation with Gazebo","description":"Master physics simulation, sensor modeling, and sim-to-real transfer for humanoid robotics using Gazebo and URDF.","source":"@site/docs/chapters/ch4-sim.mdx","sourceDirName":"chapters","slug":"/chapters/ch4-sim","permalink":"/docs/chapters/ch4-sim","draft":false,"unlisted":false,"editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs/chapters/ch4-sim.mdx","tags":[],"version":"current","frontMatter":{"id":"ch4-sim","title":"Chapter 4: Digital Twin Simulation with Gazebo","sidebar_label":"Ch4: Simulation","description":"Master physics simulation, sensor modeling, and sim-to-real transfer for humanoid robotics using Gazebo and URDF.","keywords":["gazebo","urdf","simulation","digital twin","sim-to-real","physics engine","sensor simulation","domain randomization"],"image":"/img/ch4-sim-hero.png"}},{"id":"chapters/ch5-vla","title":"Chapter 5: Vision-Language-Action Systems","description":"Integrate large language models (Llama 3) with vision encoders to ground abstract language commands into safe, executable robot trajectories.","source":"@site/docs/chapters/ch5-vla.mdx","sourceDirName":"chapters","slug":"/chapters/ch5-vla","permalink":"/docs/chapters/ch5-vla","draft":false,"unlisted":false,"editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs/chapters/ch5-vla.mdx","tags":[],"version":"current","frontMatter":{"id":"ch5-vla","title":"Chapter 5: Vision-Language-Action Systems","sidebar_label":"Ch5: VLA Systems","description":"Integrate large language models (Llama 3) with vision encoders to ground abstract language commands into safe, executable robot trajectories.","keywords":["vision-language models","vla","clip","llama 3","ollama","action grounding","language grounding","robotics"],"image":"/img/ch5-vla-hero.png"}},{"id":"chapters/ch6-capstone","title":"Chapter 6: Capstone – AI-Robot Pipeline in Production","description":"Integrate all components (kinematics, ROS 2, simulation, vision-language models) into a production-ready AI-robot system with Docker, monitoring, and real-world validation.","source":"@site/docs/chapters/ch6-capstone.mdx","sourceDirName":"chapters","slug":"/chapters/ch6-capstone","permalink":"/docs/chapters/ch6-capstone","draft":false,"unlisted":false,"editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs/chapters/ch6-capstone.mdx","tags":[],"version":"current","frontMatter":{"id":"ch6-capstone","title":"Chapter 6: Capstone – AI-Robot Pipeline in Production","sidebar_label":"Ch6: AI-Robot Pipeline","description":"Integrate all components (kinematics, ROS 2, simulation, vision-language models) into a production-ready AI-robot system with Docker, monitoring, and real-world validation.","keywords":["deployment","docker","real-world","validation","production","monitoring","humanoid","end-to-end"],"image":"/img/ch6-capstone-hero.png"}},{"id":"modules/m1-ros2/m1-nodes-topics-services","title":"Lesson 1: ROS 2 Nodes, Topics, and Services","description":"Master the core building blocks of ROS 2: nodes (independent processes), topics (pub-sub communication), and services (request-response).","source":"@site/docs/modules/m1-ros2/m1-nodes-topics-services.mdx","sourceDirName":"modules/m1-ros2","slug":"/modules/m1-ros2/m1-nodes-topics-services","permalink":"/docs/modules/m1-ros2/m1-nodes-topics-services","draft":false,"unlisted":false,"editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs/modules/m1-ros2/m1-nodes-topics-services.mdx","tags":[],"version":"current","frontMatter":{"id":"m1-nodes-topics-services","title":"Lesson 1: ROS 2 Nodes, Topics, and Services","sidebar_label":"L1: Nodes, Topics, Services","description":"Master the core building blocks of ROS 2: nodes (independent processes), topics (pub-sub communication), and services (request-response)."},"sidebar":"tutorialSidebar","previous":{"title":"M1: Overview","permalink":"/docs/modules/m1-ros2/m1-overview"},"next":{"title":"L2: Python Agents to ROS","permalink":"/docs/modules/m1-ros2/m1-python-agents"}},{"id":"modules/m1-ros2/m1-overview","title":"Module 1 Overview: The Robotic Nervous System (ROS 2)","description":"Understand ROS 2 as the middleware that connects all components of a humanoid robot. This module covers nodes, topics, services, and how to bridge Python agents to robotic hardware.","source":"@site/docs/modules/m1-ros2/m1-overview.mdx","sourceDirName":"modules/m1-ros2","slug":"/modules/m1-ros2/m1-overview","permalink":"/docs/modules/m1-ros2/m1-overview","draft":false,"unlisted":false,"editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs/modules/m1-ros2/m1-overview.mdx","tags":[],"version":"current","frontMatter":{"id":"m1-overview","title":"Module 1 Overview: The Robotic Nervous System (ROS 2)","sidebar_label":"M1: Overview","description":"Understand ROS 2 as the middleware that connects all components of a humanoid robot. This module covers nodes, topics, services, and how to bridge Python agents to robotic hardware."},"sidebar":"tutorialSidebar","previous":{"title":"Ch1: Intro to Physical AI","permalink":"/docs/chapters/ch1-intro"},"next":{"title":"L1: Nodes, Topics, Services","permalink":"/docs/modules/m1-ros2/m1-nodes-topics-services"}},{"id":"modules/m1-ros2/m1-python-agents","title":"Lesson 2: Bridging Python Agents to ROS Controllers","description":"Learn how to bridge AI agents (LLMs, planning algorithms) running in Python to actual ROS 2 robot controllers. From voice commands to executed trajectories.","source":"@site/docs/modules/m1-ros2/m1-python-agents.mdx","sourceDirName":"modules/m1-ros2","slug":"/modules/m1-ros2/m1-python-agents","permalink":"/docs/modules/m1-ros2/m1-python-agents","draft":false,"unlisted":false,"editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs/modules/m1-ros2/m1-python-agents.mdx","tags":[],"version":"current","frontMatter":{"id":"m1-python-agents","title":"Lesson 2: Bridging Python Agents to ROS Controllers","sidebar_label":"L2: Python Agents to ROS","description":"Learn how to bridge AI agents (LLMs, planning algorithms) running in Python to actual ROS 2 robot controllers. From voice commands to executed trajectories."},"sidebar":"tutorialSidebar","previous":{"title":"L1: Nodes, Topics, Services","permalink":"/docs/modules/m1-ros2/m1-nodes-topics-services"},"next":{"title":"L3: URDF Humanoids","permalink":"/docs/modules/m1-ros2/m1-urdf-humanoids"}},{"id":"modules/m1-ros2/m1-urdf-humanoids","title":"Lesson 3: URDF for Humanoids","description":"Master URDF (Unified Robot Description Format) - the XML language for defining robot structures. Learn to model humanoid kinematic chains, joint limits, and physical properties.","source":"@site/docs/modules/m1-ros2/m1-urdf-humanoids.mdx","sourceDirName":"modules/m1-ros2","slug":"/modules/m1-ros2/m1-urdf-humanoids","permalink":"/docs/modules/m1-ros2/m1-urdf-humanoids","draft":false,"unlisted":false,"editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs/modules/m1-ros2/m1-urdf-humanoids.mdx","tags":[],"version":"current","frontMatter":{"id":"m1-urdf-humanoids","title":"Lesson 3: URDF for Humanoids","sidebar_label":"L3: URDF Humanoids","description":"Master URDF (Unified Robot Description Format) - the XML language for defining robot structures. Learn to model humanoid kinematic chains, joint limits, and physical properties."},"sidebar":"tutorialSidebar","previous":{"title":"L2: Python Agents to ROS","permalink":"/docs/modules/m1-ros2/m1-python-agents"},"next":{"title":"M2: Overview","permalink":"/docs/modules/m2-digital-twin/m2-overview"}},{"id":"modules/m2-digital-twin/m2-gazebo-physics","title":"Lesson 1: Gazebo Physics Simulation","description":"Master Gazebo for physics-accurate robot simulation. Configure physics engines, set up worlds, and simulate complex interactions.","source":"@site/docs/modules/m2-digital-twin/m2-gazebo-physics.mdx","sourceDirName":"modules/m2-digital-twin","slug":"/modules/m2-digital-twin/m2-gazebo-physics","permalink":"/docs/modules/m2-digital-twin/m2-gazebo-physics","draft":false,"unlisted":false,"editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs/modules/m2-digital-twin/m2-gazebo-physics.mdx","tags":[],"version":"current","frontMatter":{"id":"m2-gazebo-physics","title":"Lesson 1: Gazebo Physics Simulation","sidebar_label":"L1: Gazebo Physics","description":"Master Gazebo for physics-accurate robot simulation. Configure physics engines, set up worlds, and simulate complex interactions."},"sidebar":"tutorialSidebar","previous":{"title":"M2: Overview","permalink":"/docs/modules/m2-digital-twin/m2-overview"},"next":{"title":"L2: Unity Rendering","permalink":"/docs/modules/m2-digital-twin/m2-unity-rendering"}},{"id":"modules/m2-digital-twin/m2-overview","title":"Module 2 Overview: The Digital Twin (Gazebo & Unity)","description":"Master physics simulation with Gazebo and high-fidelity visualization with Unity. Simulate complex environments, sensor dynamics, and human-robot interaction.","source":"@site/docs/modules/m2-digital-twin/m2-overview.mdx","sourceDirName":"modules/m2-digital-twin","slug":"/modules/m2-digital-twin/m2-overview","permalink":"/docs/modules/m2-digital-twin/m2-overview","draft":false,"unlisted":false,"editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs/modules/m2-digital-twin/m2-overview.mdx","tags":[],"version":"current","frontMatter":{"id":"m2-overview","title":"Module 2 Overview: The Digital Twin (Gazebo & Unity)","sidebar_label":"M2: Overview","description":"Master physics simulation with Gazebo and high-fidelity visualization with Unity. Simulate complex environments, sensor dynamics, and human-robot interaction."},"sidebar":"tutorialSidebar","previous":{"title":"L3: URDF Humanoids","permalink":"/docs/modules/m1-ros2/m1-urdf-humanoids"},"next":{"title":"L1: Gazebo Physics","permalink":"/docs/modules/m2-digital-twin/m2-gazebo-physics"}},{"id":"modules/m2-digital-twin/m2-sensor-simulation","title":"Lesson 3: Sensor Simulation","description":"Simulate realistic sensors in Gazebo: cameras (RGB/depth), IMU, force/torque sensors. Add noise and calibrate for sim-to-real transfer.","source":"@site/docs/modules/m2-digital-twin/m2-sensor-simulation.mdx","sourceDirName":"modules/m2-digital-twin","slug":"/modules/m2-digital-twin/m2-sensor-simulation","permalink":"/docs/modules/m2-digital-twin/m2-sensor-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs/modules/m2-digital-twin/m2-sensor-simulation.mdx","tags":[],"version":"current","frontMatter":{"id":"m2-sensor-simulation","title":"Lesson 3: Sensor Simulation","sidebar_label":"L3: Sensor Simulation","description":"Simulate realistic sensors in Gazebo: cameras (RGB/depth), IMU, force/torque sensors. Add noise and calibrate for sim-to-real transfer."},"sidebar":"tutorialSidebar","previous":{"title":"L2: Unity Rendering","permalink":"/docs/modules/m2-digital-twin/m2-unity-rendering"},"next":{"title":"M3: Overview","permalink":"/docs/modules/m3-isaac/m3-overview"}},{"id":"modules/m2-digital-twin/m2-unity-rendering","title":"Lesson 2: Unity Rendering & Visualization","description":"Build photorealistic robot visualizations in Unity. Stream sensor data from Gazebo and create interactive dashboards for humanoid control.","source":"@site/docs/modules/m2-digital-twin/m2-unity-rendering.mdx","sourceDirName":"modules/m2-digital-twin","slug":"/modules/m2-digital-twin/m2-unity-rendering","permalink":"/docs/modules/m2-digital-twin/m2-unity-rendering","draft":false,"unlisted":false,"editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs/modules/m2-digital-twin/m2-unity-rendering.mdx","tags":[],"version":"current","frontMatter":{"id":"m2-unity-rendering","title":"Lesson 2: Unity Rendering & Visualization","sidebar_label":"L2: Unity Rendering","description":"Build photorealistic robot visualizations in Unity. Stream sensor data from Gazebo and create interactive dashboards for humanoid control."},"sidebar":"tutorialSidebar","previous":{"title":"L1: Gazebo Physics","permalink":"/docs/modules/m2-digital-twin/m2-gazebo-physics"},"next":{"title":"L3: Sensor Simulation","permalink":"/docs/modules/m2-digital-twin/m2-sensor-simulation"}},{"id":"modules/m3-isaac/m3-isaac-ros","title":"Lesson 2: Isaac ROS (GPU-Accelerated Perception)","description":"Implement hardware-accelerated perception with Isaac ROS: VSLAM, object detection, depth estimation.","source":"@site/docs/modules/m3-isaac/m3-isaac-ros.mdx","sourceDirName":"modules/m3-isaac","slug":"/modules/m3-isaac/m3-isaac-ros","permalink":"/docs/modules/m3-isaac/m3-isaac-ros","draft":false,"unlisted":false,"editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs/modules/m3-isaac/m3-isaac-ros.mdx","tags":[],"version":"current","frontMatter":{"id":"m3-isaac-ros","title":"Lesson 2: Isaac ROS (GPU-Accelerated Perception)","sidebar_label":"L2: Isaac ROS","description":"Implement hardware-accelerated perception with Isaac ROS: VSLAM, object detection, depth estimation."},"sidebar":"tutorialSidebar","previous":{"title":"L1: Isaac Sim","permalink":"/docs/modules/m3-isaac/m3-isaac-sim"},"next":{"title":"L3: Nav2 Planning","permalink":"/docs/modules/m3-isaac/m3-nav2-planning"}},{"id":"modules/m3-isaac/m3-isaac-sim","title":"Lesson 1: NVIDIA Isaac Sim","description":"Master photorealistic robot simulation with NVIDIA Isaac Sim. Generate synthetic training data at scale.","source":"@site/docs/modules/m3-isaac/m3-isaac-sim.mdx","sourceDirName":"modules/m3-isaac","slug":"/modules/m3-isaac/m3-isaac-sim","permalink":"/docs/modules/m3-isaac/m3-isaac-sim","draft":false,"unlisted":false,"editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs/modules/m3-isaac/m3-isaac-sim.mdx","tags":[],"version":"current","frontMatter":{"id":"m3-isaac-sim","title":"Lesson 1: NVIDIA Isaac Sim","sidebar_label":"L1: Isaac Sim","description":"Master photorealistic robot simulation with NVIDIA Isaac Sim. Generate synthetic training data at scale."},"sidebar":"tutorialSidebar","previous":{"title":"M3: Overview","permalink":"/docs/modules/m3-isaac/m3-overview"},"next":{"title":"L2: Isaac ROS","permalink":"/docs/modules/m3-isaac/m3-isaac-ros"}},{"id":"modules/m3-isaac/m3-nav2-planning","title":"Lesson 3: Nav2 Path Planning for Humanoids","description":"Implement autonomous navigation for bipedal humanoid robots using Nav2. Global planning, local planning, and obstacle avoidance.","source":"@site/docs/modules/m3-isaac/m3-nav2-planning.mdx","sourceDirName":"modules/m3-isaac","slug":"/modules/m3-isaac/m3-nav2-planning","permalink":"/docs/modules/m3-isaac/m3-nav2-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs/modules/m3-isaac/m3-nav2-planning.mdx","tags":[],"version":"current","frontMatter":{"id":"m3-nav2-planning","title":"Lesson 3: Nav2 Path Planning for Humanoids","sidebar_label":"L3: Nav2 Planning","description":"Implement autonomous navigation for bipedal humanoid robots using Nav2. Global planning, local planning, and obstacle avoidance."},"sidebar":"tutorialSidebar","previous":{"title":"L2: Isaac ROS","permalink":"/docs/modules/m3-isaac/m3-isaac-ros"},"next":{"title":"M4: Overview","permalink":"/docs/modules/m4-vla/m4-overview"}},{"id":"modules/m3-isaac/m3-overview","title":"Module 3 Overview: The AI-Robot Brain (NVIDIA Isaac)","description":"Master NVIDIA Isaac Sim for photorealistic simulation and Isaac ROS for hardware-accelerated perception. Build autonomous humanoid systems with Nav2 path planning.","source":"@site/docs/modules/m3-isaac/m3-overview.mdx","sourceDirName":"modules/m3-isaac","slug":"/modules/m3-isaac/m3-overview","permalink":"/docs/modules/m3-isaac/m3-overview","draft":false,"unlisted":false,"editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs/modules/m3-isaac/m3-overview.mdx","tags":[],"version":"current","frontMatter":{"id":"m3-overview","title":"Module 3 Overview: The AI-Robot Brain (NVIDIA Isaac)","sidebar_label":"M3: Overview","description":"Master NVIDIA Isaac Sim for photorealistic simulation and Isaac ROS for hardware-accelerated perception. Build autonomous humanoid systems with Nav2 path planning."},"sidebar":"tutorialSidebar","previous":{"title":"L3: Sensor Simulation","permalink":"/docs/modules/m2-digital-twin/m2-sensor-simulation"},"next":{"title":"L1: Isaac Sim","permalink":"/docs/modules/m3-isaac/m3-isaac-sim"}},{"id":"modules/m4-vla/m4-capstone-project","title":"Lesson 3: Capstone Project - Autonomous Humanoid","description":"Build a complete end-to-end autonomous humanoid system. Voice → Perception → Planning → Execution.","source":"@site/docs/modules/m4-vla/m4-capstone-project.mdx","sourceDirName":"modules/m4-vla","slug":"/modules/m4-vla/m4-capstone-project","permalink":"/docs/modules/m4-vla/m4-capstone-project","draft":false,"unlisted":false,"editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs/modules/m4-vla/m4-capstone-project.mdx","tags":[],"version":"current","frontMatter":{"id":"m4-capstone-project","title":"Lesson 3: Capstone Project - Autonomous Humanoid","sidebar_label":"L3: Capstone Project","description":"Build a complete end-to-end autonomous humanoid system. Voice → Perception → Planning → Execution."},"sidebar":"tutorialSidebar","previous":{"title":"L2: Cognitive Planning","permalink":"/docs/modules/m4-vla/m4-cognitive-planning"}},{"id":"modules/m4-vla/m4-cognitive-planning","title":"Lesson 2: Cognitive Planning with LLMs","description":"Use large language models to translate natural language into step-by-step robot plans. Handle ambiguity, ask clarifications, ensure safety.","source":"@site/docs/modules/m4-vla/m4-cognitive-planning.mdx","sourceDirName":"modules/m4-vla","slug":"/modules/m4-vla/m4-cognitive-planning","permalink":"/docs/modules/m4-vla/m4-cognitive-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs/modules/m4-vla/m4-cognitive-planning.mdx","tags":[],"version":"current","frontMatter":{"id":"m4-cognitive-planning","title":"Lesson 2: Cognitive Planning with LLMs","sidebar_label":"L2: Cognitive Planning","description":"Use large language models to translate natural language into step-by-step robot plans. Handle ambiguity, ask clarifications, ensure safety."},"sidebar":"tutorialSidebar","previous":{"title":"L1: Voice-to-Action","permalink":"/docs/modules/m4-vla/m4-voice-to-action"},"next":{"title":"L3: Capstone Project","permalink":"/docs/modules/m4-vla/m4-capstone-project"}},{"id":"modules/m4-vla/m4-overview","title":"Module 4 Overview: Vision-Language-Action (VLA)","description":"Master the convergence of large language models and robotics. Build systems where robots understand natural language commands and execute complex tasks.","source":"@site/docs/modules/m4-vla/m4-overview.mdx","sourceDirName":"modules/m4-vla","slug":"/modules/m4-vla/m4-overview","permalink":"/docs/modules/m4-vla/m4-overview","draft":false,"unlisted":false,"editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs/modules/m4-vla/m4-overview.mdx","tags":[],"version":"current","frontMatter":{"id":"m4-overview","title":"Module 4 Overview: Vision-Language-Action (VLA)","sidebar_label":"M4: Overview","description":"Master the convergence of large language models and robotics. Build systems where robots understand natural language commands and execute complex tasks."},"sidebar":"tutorialSidebar","previous":{"title":"L3: Nav2 Planning","permalink":"/docs/modules/m3-isaac/m3-nav2-planning"},"next":{"title":"L1: Voice-to-Action","permalink":"/docs/modules/m4-vla/m4-voice-to-action"}},{"id":"modules/m4-vla/m4-voice-to-action","title":"Lesson 1: Voice-to-Action with Whisper","description":"Build real-time speech recognition for robots using OpenAI Whisper. Convert voice commands into actionable ROS 2 messages.","source":"@site/docs/modules/m4-vla/m4-voice-to-action.mdx","sourceDirName":"modules/m4-vla","slug":"/modules/m4-vla/m4-voice-to-action","permalink":"/docs/modules/m4-vla/m4-voice-to-action","draft":false,"unlisted":false,"editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs/modules/m4-vla/m4-voice-to-action.mdx","tags":[],"version":"current","frontMatter":{"id":"m4-voice-to-action","title":"Lesson 1: Voice-to-Action with Whisper","sidebar_label":"L1: Voice-to-Action","description":"Build real-time speech recognition for robots using OpenAI Whisper. Convert voice commands into actionable ROS 2 messages."},"sidebar":"tutorialSidebar","previous":{"title":"M4: Overview","permalink":"/docs/modules/m4-vla/m4-overview"},"next":{"title":"L2: Cognitive Planning","permalink":"/docs/modules/m4-vla/m4-cognitive-planning"}}],"drafts":[],"sidebars":{"tutorialSidebar":[{"type":"category","label":"Front Matter","items":[{"type":"doc","id":"chapters/ch0-preface"},{"type":"doc","id":"chapters/ch0-prerequisites"},{"type":"doc","id":"chapters/ch1-intro"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 1: The Robotic Nervous System (ROS 2)","items":[{"type":"doc","id":"modules/m1-ros2/m1-overview"},{"type":"doc","id":"modules/m1-ros2/m1-nodes-topics-services"},{"type":"doc","id":"modules/m1-ros2/m1-python-agents"},{"type":"doc","id":"modules/m1-ros2/m1-urdf-humanoids"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 2: The Digital Twin (Gazebo & Unity)","items":[{"type":"doc","id":"modules/m2-digital-twin/m2-overview"},{"type":"doc","id":"modules/m2-digital-twin/m2-gazebo-physics"},{"type":"doc","id":"modules/m2-digital-twin/m2-unity-rendering"},{"type":"doc","id":"modules/m2-digital-twin/m2-sensor-simulation"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 3: The AI-Robot Brain (NVIDIA Isaac)","items":[{"type":"doc","id":"modules/m3-isaac/m3-overview"},{"type":"doc","id":"modules/m3-isaac/m3-isaac-sim"},{"type":"doc","id":"modules/m3-isaac/m3-isaac-ros"},{"type":"doc","id":"modules/m3-isaac/m3-nav2-planning"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 4: Vision-Language-Action (VLA)","items":[{"type":"doc","id":"modules/m4-vla/m4-overview"},{"type":"doc","id":"modules/m4-vla/m4-voice-to-action"},{"type":"doc","id":"modules/m4-vla/m4-cognitive-planning"},{"type":"doc","id":"modules/m4-vla/m4-capstone-project"}],"collapsed":true,"collapsible":true}]}}]}},"docusaurus-plugin-content-pages":{"default":[{"type":"jsx","permalink":"/","source":"@site/src/pages/index.js"}]},"docusaurus-plugin-debug":{},"docusaurus-plugin-svgr":{},"docusaurus-theme-classic":{},"docusaurus-bootstrap-plugin":{},"docusaurus-mdx-fallback-plugin":{}}}