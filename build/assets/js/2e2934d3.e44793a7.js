"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_textbook=self.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[177],{8453:(n,e,r)=>{r.d(e,{R:()=>i,x:()=>a});var o=r(6540);const t={},s=o.createContext(t);function i(n){const e=o.useContext(s);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:i(n.components),o.createElement(s.Provider,{value:e},n.children)}},8662:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>i,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"modules/m1-ros2/m1-python-agents","title":"Lesson 2: Bridging Python Agents to ROS Controllers","description":"Learn how to bridge AI agents (LLMs, planning algorithms) running in Python to actual ROS 2 robot controllers. From voice commands to executed trajectories.","source":"@site/docs/modules/m1-ros2/m1-python-agents.mdx","sourceDirName":"modules/m1-ros2","slug":"/modules/m1-ros2/m1-python-agents","permalink":"/docs/modules/m1-ros2/m1-python-agents","draft":false,"unlisted":false,"editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs/modules/m1-ros2/m1-python-agents.mdx","tags":[],"version":"current","frontMatter":{"id":"m1-python-agents","title":"Lesson 2: Bridging Python Agents to ROS Controllers","sidebar_label":"L2: Python Agents to ROS","description":"Learn how to bridge AI agents (LLMs, planning algorithms) running in Python to actual ROS 2 robot controllers. From voice commands to executed trajectories."},"sidebar":"tutorialSidebar","previous":{"title":"L1: Nodes, Topics, Services","permalink":"/docs/modules/m1-ros2/m1-nodes-topics-services"},"next":{"title":"L3: URDF Humanoids","permalink":"/docs/modules/m1-ros2/m1-urdf-humanoids"}}');var t=r(4848),s=r(8453);const i={id:"m1-python-agents",title:"Lesson 2: Bridging Python Agents to ROS Controllers",sidebar_label:"L2: Python Agents to ROS",description:"Learn how to bridge AI agents (LLMs, planning algorithms) running in Python to actual ROS 2 robot controllers. From voice commands to executed trajectories."},a=void 0,l={},c=[{value:"The Agent-Controller Bridge",id:"the-agent-controller-bridge",level:2},{value:"Architecture: Agent Node",id:"architecture-agent-node",level:2},{value:"Example: Voice Command \u2192 Robot Arm Movement",id:"example-voice-command--robot-arm-movement",level:3},{value:"Using Local LLMs with Ollama",id:"using-local-llms-with-ollama",level:2},{value:"Safety Considerations",id:"safety-considerations",level:2},{value:"Real-World Example: Tesla Optimus",id:"real-world-example-tesla-optimus",level:2},{value:"Hands-On Project",id:"hands-on-project",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(n){const e={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.h2,{id:"the-agent-controller-bridge",children:"The Agent-Controller Bridge"}),"\n",(0,t.jsxs)(e.p,{children:["In modern humanoid robots, the ",(0,t.jsx)(e.strong,{children:"AI agent"})," and ",(0,t.jsx)(e.strong,{children:"robot controller"})," are separate systems that must communicate:"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:" AI Agent (Llama 3, GPT, etc.)\r\n   \u2193\r\n   [ROS 2 Bridge Node]\r\n   \u2193\r\n Robot Controller (ROS 2 nodes)\r\n   \u2193\r\n Actuators (Motors, Grippers)\n"})}),"\n",(0,t.jsx)(e.p,{children:"This lesson teaches you how to build that bridge."}),"\n",(0,t.jsx)(e.h2,{id:"architecture-agent-node",children:"Architecture: Agent Node"}),"\n",(0,t.jsxs)(e.p,{children:["An ",(0,t.jsx)(e.strong,{children:"agent node"})," is a ROS 2 node that:"]}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Listens"})," to natural language commands (voice, text, or LLM outputs)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Processes"})," them with an AI model"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Publishes"})," robot commands (joint angles, trajectories, gripper signals)"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"example-voice-command--robot-arm-movement",children:"Example: Voice Command \u2192 Robot Arm Movement"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom geometry_msgs.msg import Pose\r\nfrom trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint\r\nimport numpy as np\r\nfrom openai import OpenAI  # Or use local Llama 3 via Ollama\r\n\r\nclass VoiceCommandAgent(Node):\r\n    def __init__(self):\r\n        super().__init__(\'voice_command_agent\')\r\n\r\n        # Publisher to robot arm\r\n        self.arm_publisher = self.create_publisher(\r\n            JointTrajectory,\r\n            \'/arm/controller/command\',\r\n            10\r\n        )\r\n\r\n        # Subscribe to voice commands (from speech-to-text node)\r\n        self.voice_subscription = self.create_subscription(\r\n            String,\r\n            \'/voice/command\',\r\n            self.voice_callback,\r\n            10\r\n        )\r\n\r\n        # Initialize LLM client\r\n        self.client = OpenAI(api_key="your-key")\r\n\r\n        self.get_logger().info("Voice command agent initialized")\r\n\r\n    def voice_callback(self, msg):\r\n        """Called when a voice command is received."""\r\n        command = msg.data\r\n        self.get_logger().info(f"Received voice command: {command}")\r\n\r\n        # Step 1: Use LLM to translate voice to action\r\n        action = self.plan_action_with_llm(command)\r\n\r\n        # Step 2: Translate action to joint angles\r\n        trajectory = self.action_to_trajectory(action)\r\n\r\n        # Step 3: Publish to robot controller\r\n        self.arm_publisher.publish(trajectory)\r\n        self.get_logger().info(f"Executed action: {action}")\r\n\r\n    def plan_action_with_llm(self, command: str) -> dict:\r\n        """Use an LLM to parse the voice command."""\r\n        prompt = f"""\r\n        A robot hears the voice command: "{command}"\r\n\r\n        Respond with a JSON object describing the action:\r\n        {{\r\n            "action": "move_arm" | "grasp" | "push" | "place",\r\n            "target_position": {{"x": float, "y": float, "z": float}},\r\n            "gripper_state": "open" | "closed",\r\n            "duration_seconds": float\r\n        }}\r\n\r\n        Only output the JSON, nothing else.\r\n        """\r\n\r\n        response = self.client.chat.completions.create(\r\n            model="gpt-4",\r\n            messages=[{"role": "user", "content": prompt}],\r\n            temperature=0.3\r\n        )\r\n\r\n        import json\r\n        action_json = response.choices[0].message.content\r\n        action = json.loads(action_json)\r\n\r\n        return action\r\n\r\n    def action_to_trajectory(self, action: dict) -> JointTrajectory:\r\n        """Convert a semantic action to a joint trajectory."""\r\n        # This would typically use inverse kinematics\r\n        # For now, a simple example:\r\n\r\n        trajectory = JointTrajectory()\r\n        trajectory.header.stamp = self.get_clock().now().to_msg()\r\n        trajectory.joint_names = [\'shoulder_pan\', \'shoulder_lift\', \'elbow_flex\', \'wrist_1\', \'wrist_2\', \'wrist_3\']\r\n\r\n        # Simple IK: (would be much more complex in reality)\r\n        target_pos = action[\'target_position\']\r\n        q_target = self.simple_ik(target_pos[\'x\'], target_pos[\'y\'], target_pos[\'z\'])\r\n\r\n        point = JointTrajectoryPoint()\r\n        point.positions = q_target\r\n        point.time_from_start = rclpy.time.Duration(seconds=action[\'duration_seconds\']).to_msg()\r\n\r\n        trajectory.points.append(point)\r\n\r\n        return trajectory\r\n\r\n    def simple_ik(self, x, y, z):\r\n        """Simplified inverse kinematics for 6-DOF arm."""\r\n        # In reality, this would use a proper IK solver (e.g., scipy.optimize)\r\n        q = np.zeros(6)\r\n        q[0] = np.arctan2(y, x)  # Base rotation\r\n        # ... more complex computation ...\r\n        return q.tolist()\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    agent = VoiceCommandAgent()\r\n    rclpy.spin(agent)\r\n    agent.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,t.jsx)(e.h2,{id:"using-local-llms-with-ollama",children:"Using Local LLMs with Ollama"}),"\n",(0,t.jsxs)(e.p,{children:["To avoid cloud API costs and latency, use ",(0,t.jsx)(e.strong,{children:"Ollama"})," to run Llama 3 locally:"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import requests\r\nimport json\r\n\r\nclass LocalLlamaAgent(Node):\r\n    def __init__(self):\r\n        super().__init__(\'local_llama_agent\')\r\n        self.ollama_url = "http://localhost:11434/api/generate"\r\n\r\n    def plan_action_with_local_llm(self, command: str) -> dict:\r\n        """Use local Llama 3 via Ollama."""\r\n        prompt = f"""\r\n        A humanoid robot receives a voice command: "{command}"\r\n\r\n        Output a JSON object:\r\n        {{\r\n            "action": "move_arm" | "walk" | "pick_up" | "place",\r\n            "target": "location/object description",\r\n            "confidence": 0.0-1.0\r\n        }}\r\n        """\r\n\r\n        response = requests.post(\r\n            self.ollama_url,\r\n            json={\r\n                "model": "llama3",\r\n                "prompt": prompt,\r\n                "stream": False\r\n            }\r\n        )\r\n\r\n        # Parse response\r\n        output = response.json()[\'response\']\r\n\r\n        import json\r\n        action = json.loads(output)\r\n\r\n        return action\n'})}),"\n",(0,t.jsx)(e.h2,{id:"safety-considerations",children:"Safety Considerations"}),"\n",(0,t.jsxs)(e.p,{children:["When bridging AI agents to robot controllers, ",(0,t.jsx)(e.strong,{children:"safety is critical"}),":"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"class SafeAgentController(Node):\r\n    def __init__(self):\r\n        super().__init__('safe_agent_controller')\r\n\r\n        # Define safe workspace bounds\r\n        self.workspace_bounds = {\r\n            'x': (-0.5, 1.5),   # meters\r\n            'y': (-1.0, 1.0),\r\n            'z': (0.0, 2.0)\r\n        }\r\n\r\n        self.max_joint_velocity = 1.5  # rad/s\r\n\r\n    def is_action_safe(self, action: dict) -> bool:\r\n        \"\"\"Validate action before sending to robot.\"\"\"\r\n        target = action['target_position']\r\n\r\n        # Check workspace bounds\r\n        if not (self.workspace_bounds['x'][0] <= target['x'] <= self.workspace_bounds['x'][1]):\r\n            return False\r\n        if not (self.workspace_bounds['y'][0] <= target['y'] <= self.workspace_bounds['y'][1]):\r\n            return False\r\n        if not (self.workspace_bounds['z'][0] <= target['z'] <= self.workspace_bounds['z'][1]):\r\n            return False\r\n\r\n        # Check for dangerous keywords\r\n        dangerous_words = ['destroy', 'break', 'harm', 'hurt']\r\n        for word in dangerous_words:\r\n            if word in action.get('reasoning', '').lower():\r\n                return False\r\n\r\n        return True\r\n\r\n    def execute_action(self, action: dict):\r\n        \"\"\"Execute only if safe.\"\"\"\r\n        if not self.is_action_safe(action):\r\n            self.get_logger().error(f\"Action rejected: {action}\")\r\n            return\r\n\r\n        # Safe to execute\r\n        trajectory = self.action_to_trajectory(action)\r\n        self.arm_publisher.publish(trajectory)\n"})}),"\n",(0,t.jsx)(e.h2,{id:"real-world-example-tesla-optimus",children:"Real-World Example: Tesla Optimus"}),"\n",(0,t.jsx)(e.p,{children:"Tesla Optimus uses a similar architecture:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Perception node"}),": Cameras \u2192 scene understanding"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Planning node"}),": LLM-based task planner"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Control node"}),": Converts planned actions to motor commands"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Safety monitor"}),": Ensures no collisions or constraint violations"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"hands-on-project",children:"Hands-On Project"}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Build a robot arm controller that:"})}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Listens to voice commands via a speech-to-text node"}),"\n",(0,t.jsx)(e.li,{children:"Uses Llama 3 (via Ollama) to interpret the command"}),"\n",(0,t.jsx)(e.li,{children:"Computes inverse kinematics to target position"}),"\n",(0,t.jsx)(e.li,{children:"Publishes a smooth trajectory to the robot"}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Expected voice commands:"})}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:'"Move the arm to the table"'}),"\n",(0,t.jsx)(e.li,{children:'"Pick up the red cube"'}),"\n",(0,t.jsx)(e.li,{children:'"Place the object in the bin"'}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Deliverables:"})}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"A working Python ROS 2 node"}),"\n",(0,t.jsx)(e.li,{children:"Safety validation for all actions"}),"\n",(0,t.jsx)(e.li,{children:"Graceful error handling when the LLM response is invalid"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"/docs/modules/m1-ros2/m1-urdf-humanoids",children:"Lesson 3: URDF for Humanoids"})}),"\n",(0,t.jsx)(e.li,{children:"Experiment with different LLMs (Llama 2, Mistral, etc.)"}),"\n",(0,t.jsx)(e.li,{children:"Add multi-robot coordination (multiple arms, mobile base)"}),"\n"]})]})}function m(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}}}]);