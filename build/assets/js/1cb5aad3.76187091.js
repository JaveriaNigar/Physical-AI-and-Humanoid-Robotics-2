"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_textbook=self.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[805],{4014:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>m,frontMatter:()=>a,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"modules/m3-isaac/m3-isaac-ros","title":"Lesson 2: Isaac ROS (GPU-Accelerated Perception)","description":"Implement hardware-accelerated perception with Isaac ROS: VSLAM, object detection, depth estimation.","source":"@site/docs/modules/m3-isaac/m3-isaac-ros.mdx","sourceDirName":"modules/m3-isaac","slug":"/modules/m3-isaac/m3-isaac-ros","permalink":"/docs/modules/m3-isaac/m3-isaac-ros","draft":false,"unlisted":false,"editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs/modules/m3-isaac/m3-isaac-ros.mdx","tags":[],"version":"current","frontMatter":{"id":"m3-isaac-ros","title":"Lesson 2: Isaac ROS (GPU-Accelerated Perception)","sidebar_label":"L2: Isaac ROS","description":"Implement hardware-accelerated perception with Isaac ROS: VSLAM, object detection, depth estimation."},"sidebar":"tutorialSidebar","previous":{"title":"L1: Isaac Sim","permalink":"/docs/modules/m3-isaac/m3-isaac-sim"},"next":{"title":"L3: Nav2 Planning","permalink":"/docs/modules/m3-isaac/m3-nav2-planning"}}');var t=r(4848),o=r(8453);const a={id:"m3-isaac-ros",title:"Lesson 2: Isaac ROS (GPU-Accelerated Perception)",sidebar_label:"L2: Isaac ROS",description:"Implement hardware-accelerated perception with Isaac ROS: VSLAM, object detection, depth estimation."},i=void 0,c={},l=[{value:"What is Isaac ROS?",id:"what-is-isaac-ros",level:2},{value:"Installation",id:"installation",level:2},{value:"Visual SLAM (VSLAM)",id:"visual-slam-vslam",level:2},{value:"Launch VSLAM Node",id:"launch-vslam-node",level:3},{value:"Subscribe to Odometry",id:"subscribe-to-odometry",level:3},{value:"Real-Time Object Detection",id:"real-time-object-detection",level:2},{value:"Stereo Depth Estimation",id:"stereo-depth-estimation",level:2},{value:"Multi-Sensor Fusion",id:"multi-sensor-fusion",level:2},{value:"Performance Metrics",id:"performance-metrics",level:2},{value:"Next Lesson",id:"next-lesson",level:2}];function d(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h2,{id:"what-is-isaac-ros",children:"What is Isaac ROS?"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Isaac ROS"})," provides GPU-accelerated perception nodes for ROS 2:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"VSLAM"}),": Visual SLAM using camera + IMU (10\xd7 faster than CPU)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Object Detection"}),": Real-time YOLOv8 on GPU"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Depth Estimation"}),": Stereo vision with GPU acceleration"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Hardware-optimized"}),": Runs on NVIDIA Jetson or data center GPUs"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"installation",children:"Installation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Install Isaac ROS with VSLAM\r\nsudo apt-get install ros-humble-isaac-ros-vslam\r\n\r\n# Install perception packages\r\nsudo apt-get install ros-humble-isaac-ros-visual-slam \\\r\n                     ros-humble-isaac-ros-object-detection \\\r\n                     ros-humble-isaac-ros-dnn-stereo-depth\n"})}),"\n",(0,t.jsx)(n.h2,{id:"visual-slam-vslam",children:"Visual SLAM (VSLAM)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"VSLAM"})," localizes the robot by tracking camera features:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:" Camera Feed \u2192 Feature Extraction \u2192 Feature Matching \u2192 Localization\r\n     \u2193\r\n     IMU (gyroscope) stabilizes rotation estimates\n"})}),"\n",(0,t.jsx)(n.h3,{id:"launch-vslam-node",children:"Launch VSLAM Node"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"# isaac_ros_vslam.launch.py\r\n\r\nfrom launch import LaunchDescription\r\nfrom launch_ros.actions import Node\r\n\r\ndef generate_launch_description():\r\n    vslam_node = Node(\r\n        package='isaac_ros_visual_slam',\r\n        executable='isaac_ros_visual_slam_node',\r\n        parameters=[\r\n            {'enable_imu_fusion': True},\r\n            {'enable_loop_closure': True},\r\n            {'num_keyframes': 10},\r\n            {'depth_image_topic': '/camera/depth'},\r\n            {'rgb_image_topic': '/camera/rgb'},\r\n            {'imu_topic': '/imu/data'},\r\n        ]\r\n    )\r\n\r\n    return LaunchDescription([vslam_node])\n"})}),"\n",(0,t.jsx)(n.h3,{id:"subscribe-to-odometry",children:"Subscribe to Odometry"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from nav_msgs.msg import Odometry\r\nimport numpy as np\r\n\r\nclass LocalizationNode(Node):\r\n    def __init__(self):\r\n        super().__init__('localization_node')\r\n        self.odom_sub = self.create_subscription(\r\n            Odometry,\r\n            '/visual_slam/odometry',\r\n            self.odometry_callback,\r\n            10\r\n        )\r\n        self.robot_pose = None\r\n\r\n    def odometry_callback(self, msg):\r\n        # Extract pose\r\n        pos = msg.pose.pose.position\r\n        orient = msg.pose.pose.orientation\r\n\r\n        self.robot_pose = {\r\n            'x': pos.x, 'y': pos.y, 'z': pos.z,\r\n            'qx': orient.x, 'qy': orient.y, 'qz': orient.z, 'qw': orient.w\r\n        }\r\n\r\n        self.get_logger().info(f\"Robot at: ({pos.x:.2f}, {pos.y:.2f}, {pos.z:.2f})\")\n"})}),"\n",(0,t.jsx)(n.h2,{id:"real-time-object-detection",children:"Real-Time Object Detection"}),"\n",(0,t.jsx)(n.p,{children:"Use YOLOv8 for object detection:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from isaac_ros_object_detection import DetectionNode\r\nfrom vision_msgs.msg import Detection2DArray\r\n\r\nclass ObjectDetector(Node):\r\n    def __init__(self):\r\n        super().__init__('object_detector')\r\n        self.det_sub = self.create_subscription(\r\n            Detection2DArray,\r\n            '/detections',\r\n            self.detection_callback,\r\n            10\r\n        )\r\n\r\n    def detection_callback(self, msg):\r\n        for detection in msg.detections:\r\n            # Extract bounding box and class\r\n            bbox = detection.bbox\r\n            label = detection.results[0].hypothesis.class_name\r\n            confidence = detection.results[0].hypothesis.score\r\n\r\n            if confidence > 0.7:  # High confidence\r\n                self.get_logger().info(\r\n                    f\"Detected {label} at ({bbox.center.x:.0f}, {bbox.center.y:.0f}) \"\r\n                    f\"(confidence: {confidence:.2f})\"\r\n                )\n"})}),"\n",(0,t.jsx)(n.h2,{id:"stereo-depth-estimation",children:"Stereo Depth Estimation"}),"\n",(0,t.jsx)(n.p,{children:"Estimate 3D structure from two cameras:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"# stereo_depth.launch.py\r\n\r\ndef generate_launch_description():\r\n    stereo_depth_node = Node(\r\n        package='isaac_ros_dnn_stereo_depth',\r\n        executable='isaac_ros_dnn_stereo_depth_node',\r\n        parameters=[\r\n            {'stereo_baseline': 0.12},  # 120mm baseline\r\n            {'model': 'raft-stereo'},\r\n            {'image_width': 1280},\r\n            {'image_height': 720},\r\n        ]\r\n    )\r\n\r\n    return LaunchDescription([stereo_depth_node])\n"})}),"\n",(0,t.jsx)(n.h2,{id:"multi-sensor-fusion",children:"Multi-Sensor Fusion"}),"\n",(0,t.jsx)(n.p,{children:"Combine VSLAM, object detection, and depth for robust perception:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import tf_transformations\r\nfrom geometry_msgs.msg import TransformStamped\r\n\r\nclass PerceptionFusion(Node):\r\n    def __init__(self):\r\n        super().__init__(\'perception_fusion\')\r\n\r\n        # Subscribers\r\n        self.vslam_sub = self.create_subscription(Odometry, \'/visual_slam/odometry\', self.vslam_cb, 10)\r\n        self.det_sub = self.create_subscription(Detection2DArray, \'/detections\', self.det_cb, 10)\r\n        self.depth_sub = self.create_subscription(Image, \'/depth\', self.depth_cb, 10)\r\n\r\n        # Publisher\r\n        self.world_pub = self.create_publisher(Marker, \'/world_objects\', 10)\r\n\r\n    def vslam_cb(self, msg):\r\n        """Update robot localization."""\r\n        self.robot_frame = msg.pose.pose\r\n\r\n    def det_cb(self, msg):\r\n        """Detect objects in camera frame."""\r\n        for detection in msg.detections:\r\n            # Project 2D detection to 3D using depth map\r\n            x_pixel = detection.bbox.center.x\r\n            y_pixel = detection.bbox.center.y\r\n            depth = self.get_depth_at(x_pixel, y_pixel)\r\n\r\n            # Transform from camera to world frame\r\n            world_point = self.camera_to_world(depth, x_pixel, y_pixel)\r\n\r\n            # Publish marker\r\n            self.publish_marker(world_point, detection.results[0].hypothesis.class_name)\r\n\r\n    def get_depth_at(self, x, y):\r\n        """Get depth at pixel location."""\r\n        # Linear interpolation from depth map\r\n        # ...\r\n        pass\r\n\r\n    def camera_to_world(self, depth, x, y):\r\n        """Transform camera coordinates to world coordinates."""\r\n        # Use robot pose and camera intrinsics\r\n        # ...\r\n        pass\n'})}),"\n",(0,t.jsx)(n.h2,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import time\r\n\r\nclass PerformanceMonitor(Node):\r\n    def __init__(self):\r\n        super().__init__('perf_monitor')\r\n        self.timer = self.create_timer(1.0, self.monitor_callback)\r\n        self.frame_times = []\r\n\r\n    def monitor_callback(self):\r\n        avg_latency = np.mean(self.frame_times[-30:]) * 1000  # ms\r\n\r\n        self.get_logger().info(\r\n            f\"VSLAM latency: {avg_latency:.1f} ms ({1000/avg_latency:.1f} FPS)\"\r\n        )\n"})}),"\n",(0,t.jsx)(n.p,{children:"Expected performance on RTX 4090:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"VSLAM"}),": 4-5 ms per frame (200 FPS)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"YOLOv8"}),": 10-15 ms per frame (70 FPS)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Stereo depth"}),": 20-30 ms per frame (33 FPS)"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"next-lesson",children:"Next Lesson"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"/docs/modules/m3-isaac/m3-nav2-planning",children:"Lesson 3: Nav2 Path Planning"})})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>i});var s=r(6540);const t={},o=s.createContext(t);function a(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);