"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_textbook=self.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[557],{3406:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>m,frontMatter:()=>s,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"modules/m4-vla/m4-voice-to-action","title":"Lesson 1: Voice-to-Action with Whisper","description":"Build real-time speech recognition for robots using OpenAI Whisper. Convert voice commands into actionable ROS 2 messages.","source":"@site/docs/modules/m4-vla/m4-voice-to-action.mdx","sourceDirName":"modules/m4-vla","slug":"/modules/m4-vla/m4-voice-to-action","permalink":"/docs/modules/m4-vla/m4-voice-to-action","draft":false,"unlisted":false,"editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs/modules/m4-vla/m4-voice-to-action.mdx","tags":[],"version":"current","frontMatter":{"id":"m4-voice-to-action","title":"Lesson 1: Voice-to-Action with Whisper","sidebar_label":"L1: Voice-to-Action","description":"Build real-time speech recognition for robots using OpenAI Whisper. Convert voice commands into actionable ROS 2 messages."},"sidebar":"tutorialSidebar","previous":{"title":"M4: Overview","permalink":"/docs/modules/m4-vla/m4-overview"},"next":{"title":"L2: Cognitive Planning","permalink":"/docs/modules/m4-vla/m4-cognitive-planning"}}');var t=r(4848),o=r(8453);const s={id:"m4-voice-to-action",title:"Lesson 1: Voice-to-Action with Whisper",sidebar_label:"L1: Voice-to-Action",description:"Build real-time speech recognition for robots using OpenAI Whisper. Convert voice commands into actionable ROS 2 messages."},a=void 0,c={},d=[{value:"Speech Recognition Pipeline",id:"speech-recognition-pipeline",level:2},{value:"Installing Whisper",id:"installing-whisper",level:2},{value:"Local Whisper Node (ROS 2)",id:"local-whisper-node-ros-2",level:2},{value:"Intent Recognition",id:"intent-recognition",level:2},{value:"Extract Parameters from Speech",id:"extract-parameters-from-speech",level:2},{value:"Real-Time Performance",id:"real-time-performance",level:2},{value:"Next Lesson",id:"next-lesson",level:2}];function l(e){const n={a:"a",code:"code",h2:"h2",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h2,{id:"speech-recognition-pipeline",children:"Speech Recognition Pipeline"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:' Microphone Input (16 kHz, mono)\r\n    \u2193\r\n[Whisper: Audio \u2192 Text]\r\n    \u2193\r\n Text Command: "Move the arm left"\r\n    \u2193\r\n[Intent Classification]\r\n    \u2193\r\n ROS Action / Service Call\n'})}),"\n",(0,t.jsx)(n.h2,{id:"installing-whisper",children:"Installing Whisper"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Install with pip\r\npip install openai-whisper\r\n\r\n# Or from source\r\ngit clone https://github.com/openai/whisper.git\r\ncd whisper && pip install -e .\r\n\r\n# Verify\r\nwhisper --version\n"})}),"\n",(0,t.jsx)(n.h2,{id:"local-whisper-node-ros-2",children:"Local Whisper Node (ROS 2)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nimport whisper\r\nimport numpy as np\r\nimport pyaudio\r\nimport threading\r\n\r\nclass WhisperNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'whisper_node\')\r\n\r\n        # Load Whisper model (base, small, medium, large)\r\n        # Larger = more accurate but slower\r\n        self.model = whisper.load_model("base")\r\n\r\n        # Publisher for recognized commands\r\n        self.command_pub = self.create_publisher(String, \'/voice/command\', 10)\r\n\r\n        # Audio recording parameters\r\n        self.CHUNK = 1024\r\n        self.FORMAT = pyaudio.paFloat32\r\n        self.CHANNELS = 1\r\n        self.RATE = 16000\r\n\r\n        self.audio_data = []\r\n        self.recording = False\r\n\r\n        # Start microphone thread\r\n        self.record_thread = threading.Thread(target=self.record_audio)\r\n        self.record_thread.daemon = True\r\n        self.record_thread.start()\r\n\r\n        self.get_logger().info("Whisper node started, listening for voice commands...")\r\n\r\n    def record_audio(self):\r\n        """Continuously record audio in background."""\r\n        p = pyaudio.PyAudio()\r\n\r\n        stream = p.open(\r\n            format=self.FORMAT,\r\n            channels=self.CHANNELS,\r\n            rate=self.RATE,\r\n            input=True,\r\n            frames_per_buffer=self.CHUNK\r\n        )\r\n\r\n        silence_duration = 0\r\n        SILENCE_THRESHOLD = 0.02\r\n        SILENCE_FRAMES = 30  # ~1.7 seconds of silence to end\r\n\r\n        while True:\r\n            data = stream.read(self.CHUNK)\r\n            audio_chunk = np.frombuffer(data, dtype=np.float32)\r\n\r\n            # Detect silence\r\n            rms = np.sqrt(np.mean(audio_chunk ** 2))\r\n\r\n            if rms < SILENCE_THRESHOLD:\r\n                silence_duration += 1\r\n            else:\r\n                silence_duration = 0\r\n                self.audio_data.extend(audio_chunk)\r\n\r\n            # If enough silence, process the utterance\r\n            if silence_duration > SILENCE_FRAMES and len(self.audio_data) > 0:\r\n                self.get_logger().info("End of speech detected, transcribing...")\r\n                self.transcribe_and_publish()\r\n                self.audio_data = []\r\n                silence_duration = 0\r\n\r\n    def transcribe_and_publish(self):\r\n        """Transcribe audio buffer and publish command."""\r\n        # Convert audio list to numpy array\r\n        audio_array = np.array(self.audio_data, dtype=np.float32)\r\n\r\n        # Normalize\r\n        if np.max(np.abs(audio_array)) > 0:\r\n            audio_array = audio_array / np.max(np.abs(audio_array))\r\n\r\n        # Transcribe with Whisper\r\n        try:\r\n            result = self.model.transcribe(\r\n                audio=audio_array,\r\n                language="en",\r\n                fp16=False  # Use float32\r\n            )\r\n\r\n            text = result["text"].strip()\r\n\r\n            if text:\r\n                self.get_logger().info(f"Transcribed: {text}")\r\n\r\n                # Publish command\r\n                msg = String()\r\n                msg.data = text\r\n                self.command_pub.publish(msg)\r\n            else:\r\n                self.get_logger().warn("No speech detected")\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f"Transcription error: {e}")\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = WhisperNode()\r\n\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"intent-recognition",children:"Intent Recognition"}),"\n",(0,t.jsx)(n.p,{children:"Classify voice commands into actions:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from enum import Enum\r\n\r\nclass RobotAction(Enum):\r\n    MOVE_ARM = \"move_arm\"\r\n    GRASP = \"grasp\"\r\n    RELEASE = \"release\"\r\n    WALK = \"walk\"\r\n    STOP = \"stop\"\r\n\r\nclass IntentRecognizer(Node):\r\n    def __init__(self):\r\n        super().__init__('intent_recognizer')\r\n\r\n        self.voice_sub = self.create_subscription(\r\n            String,\r\n            '/voice/command',\r\n            self.command_callback,\r\n            10\r\n        )\r\n\r\n        self.action_pub = self.create_publisher(String, '/robot/action', 10)\r\n\r\n    def command_callback(self, msg):\r\n        \"\"\"Parse voice command and extract intent.\"\"\"\r\n        text = msg.data.lower()\r\n\r\n        # Simple keyword-based classification\r\n        action = self.classify_command(text)\r\n\r\n        if action:\r\n            self.get_logger().info(f\"Recognized action: {action.value}\")\r\n            action_msg = String()\r\n            action_msg.data = action.value\r\n            self.action_pub.publish(action_msg)\r\n        else:\r\n            self.get_logger().warn(f\"Unknown command: {text}\")\r\n\r\n    def classify_command(self, text: str):\r\n        \"\"\"Classify command text to robot action.\"\"\"\r\n\r\n        move_keywords = ['move', 'go', 'left', 'right', 'forward', 'backward']\r\n        if any(kw in text for kw in move_keywords):\r\n            return RobotAction.MOVE_ARM\r\n\r\n        grasp_keywords = ['pick', 'grab', 'grasp', 'hold']\r\n        if any(kw in text for kw in grasp_keywords):\r\n            return RobotAction.GRASP\r\n\r\n        release_keywords = ['drop', 'release', 'let go', 'place']\r\n        if any(kw in text for kw in release_keywords):\r\n            return RobotAction.RELEASE\r\n\r\n        walk_keywords = ['walk', 'move', 'navigate', 'go to']\r\n        if any(kw in text for kw in walk_keywords):\r\n            return RobotAction.WALK\r\n\r\n        stop_keywords = ['stop', 'halt', 'freeze']\r\n        if any(kw in text for kw in stop_keywords):\r\n            return RobotAction.STOP\r\n\r\n        return None\n"})}),"\n",(0,t.jsx)(n.h2,{id:"extract-parameters-from-speech",children:"Extract Parameters from Speech"}),"\n",(0,t.jsx)(n.p,{children:"Parse parameters from voice commands:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import re\r\n\r\nclass CommandParser(Node):\r\n    def parse_movement_command(self, text: str) -> dict:\r\n        \"\"\"Extract movement parameters from command.\"\"\"\r\n\r\n        # \"Move arm 10 cm left\"\r\n        command = {'action': 'move_arm', 'direction': None, 'distance': None}\r\n\r\n        # Extract direction\r\n        directions = {'left': -1, 'right': 1, 'up': 1, 'down': -1, 'forward': 1, 'backward': -1}\r\n        for direction, sign in directions.items():\r\n            if direction in text:\r\n                command['direction'] = direction\r\n                break\r\n\r\n        # Extract distance (if mentioned)\r\n        distance_match = re.search(r'(\\d+)\\s*(cm|centimeter|meter|m)', text)\r\n        if distance_match:\r\n            distance = int(distance_match.group(1))\r\n            unit = distance_match.group(2)\r\n            if 'cm' in unit:\r\n                command['distance'] = distance / 100  # Convert to meters\r\n            else:\r\n                command['distance'] = distance\r\n\r\n        return command\n"})}),"\n",(0,t.jsx)(n.h2,{id:"real-time-performance",children:"Real-Time Performance"}),"\n",(0,t.jsx)(n.p,{children:"Whisper models and latency:"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Model"}),(0,t.jsx)(n.th,{children:"Size"}),(0,t.jsx)(n.th,{children:"Latency"}),(0,t.jsx)(n.th,{children:"Accuracy"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"tiny"})}),(0,t.jsx)(n.td,{children:"39 MB"}),(0,t.jsx)(n.td,{children:"100 ms"}),(0,t.jsx)(n.td,{children:"87%"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"base"})}),(0,t.jsx)(n.td,{children:"140 MB"}),(0,t.jsx)(n.td,{children:"200 ms"}),(0,t.jsx)(n.td,{children:"92%"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"small"})}),(0,t.jsx)(n.td,{children:"244 MB"}),(0,t.jsx)(n.td,{children:"400 ms"}),(0,t.jsx)(n.td,{children:"94%"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"medium"})}),(0,t.jsx)(n.td,{children:"769 MB"}),(0,t.jsx)(n.td,{children:"800 ms"}),(0,t.jsx)(n.td,{children:"96%"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"large"})}),(0,t.jsx)(n.td,{children:"2.9 GB"}),(0,t.jsx)(n.td,{children:"1500 ms"}),(0,t.jsx)(n.td,{children:"99%"})]})]})]}),"\n",(0,t.jsxs)(n.p,{children:["For real-time humanoid control, use ",(0,t.jsx)(n.strong,{children:"base"})," or ",(0,t.jsx)(n.strong,{children:"small"})," on edge devices."]}),"\n",(0,t.jsx)(n.h2,{id:"next-lesson",children:"Next Lesson"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"/docs/modules/m4-vla/m4-cognitive-planning",children:"Lesson 2: Cognitive Planning with LLMs"})})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>a});var i=r(6540);const t={},o=i.createContext(t);function s(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);