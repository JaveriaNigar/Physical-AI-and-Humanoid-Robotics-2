"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_textbook=self.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[314],{5535:(n,e,a)=>{a.r(e),a.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>i,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"modules/m4-vla/m4-cognitive-planning","title":"Lesson 2: Cognitive Planning with LLMs","description":"Use large language models to translate natural language into step-by-step robot plans. Handle ambiguity, ask clarifications, ensure safety.","source":"@site/docs/modules/m4-vla/m4-cognitive-planning.mdx","sourceDirName":"modules/m4-vla","slug":"/modules/m4-vla/m4-cognitive-planning","permalink":"/docs/modules/m4-vla/m4-cognitive-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs/modules/m4-vla/m4-cognitive-planning.mdx","tags":[],"version":"current","frontMatter":{"id":"m4-cognitive-planning","title":"Lesson 2: Cognitive Planning with LLMs","sidebar_label":"L2: Cognitive Planning","description":"Use large language models to translate natural language into step-by-step robot plans. Handle ambiguity, ask clarifications, ensure safety."},"sidebar":"tutorialSidebar","previous":{"title":"L1: Voice-to-Action","permalink":"/docs/modules/m4-vla/m4-voice-to-action"},"next":{"title":"L3: Capstone Project","permalink":"/docs/modules/m4-vla/m4-capstone-project"}}');var s=a(4848),o=a(8453);const i={id:"m4-cognitive-planning",title:"Lesson 2: Cognitive Planning with LLMs",sidebar_label:"L2: Cognitive Planning",description:"Use large language models to translate natural language into step-by-step robot plans. Handle ambiguity, ask clarifications, ensure safety."},r=void 0,l={},c=[{value:"LLM-Based Task Planning",id:"llm-based-task-planning",level:2},{value:"Using Llama 3 Locally",id:"using-llama-3-locally",level:2},{value:"Task Planning Node",id:"task-planning-node",level:2},{value:"Example Plan Output",id:"example-plan-output",level:2},{value:"Handling Ambiguity",id:"handling-ambiguity",level:2},{value:"Safety-Aware Planning",id:"safety-aware-planning",level:2},{value:"Next Lesson",id:"next-lesson",level:2}];function p(n){const e={a:"a",code:"code",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,o.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.h2,{id:"llm-based-task-planning",children:"LLM-Based Task Planning"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Problem"}),': "Pick up the red cube and place it on the shelf"']}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"What an LLM can do"}),":"]}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Break down into steps (pick \u2192 place)"}),"\n",(0,s.jsx)(e.li,{children:"Reason about spatial relationships (red cube, shelf)"}),"\n",(0,s.jsx)(e.li,{children:"Handle ambiguity (ask clarifying questions)"}),"\n",(0,s.jsx)(e.li,{children:"Plan contingencies (what if object is not found?)"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"using-llama-3-locally",children:"Using Llama 3 Locally"}),"\n",(0,s.jsx)(e.p,{children:"Install Ollama and run Llama 3:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"# Download Ollama\nhttps://ollama.ai\n\n# Pull Llama 3 model\nollama pull llama3\n\n# Start server (runs on localhost:11434)\nollama serve\n"})}),"\n",(0,s.jsx)(e.h2,{id:"task-planning-node",children:"Task Planning Node"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport requests\nimport json\n\nclass TaskPlannerNode(Node):\n    def __init__(self):\n        super().__init__(\'task_planner\')\n\n        # Subscriber\n        self.command_sub = self.create_subscription(\n            String,\n            \'/voice/command\',\n            self.command_callback,\n            10\n        )\n\n        # Publisher\n        self.plan_pub = self.create_publisher(String, \'/robot/plan\', 10)\n\n        # Ollama endpoint\n        self.ollama_url = "http://localhost:11434/api/generate"\n        self.model = "llama3"\n\n        self.get_logger().info("Task planner initialized")\n\n    def command_callback(self, msg):\n        """Process voice command and create task plan."""\n        command = msg.data\n        self.get_logger().info(f"Planning task: {command}")\n\n        # Use LLM to create plan\n        plan = self.create_plan_with_llm(command)\n\n        # Publish plan\n        plan_msg = String()\n        plan_msg.data = json.dumps(plan)\n        self.plan_pub.publish(plan_msg)\n\n    def create_plan_with_llm(self, command: str) -> dict:\n        """Generate step-by-step plan using Llama 3."""\n\n        prompt = f"""\nYou are a robot task planner. A humanoid robot receives this command:\n\n"{command}"\n\nThe robot has:\n- A 6-DOF arm with gripper\n- Cameras for vision\n- VSLAM for localization\n- Nav2 for navigation\n- ROS 2 controllers\n\nGenerate a JSON plan with the following structure:\n{{\n    "task": "task name",\n    "steps": [\n        {{\n            "step_number": 1,\n            "description": "First step",\n            "ros_action": "move_arm" | "navigate" | "grasp" | "release",\n            "parameters": {{}},\n            "preconditions": "what must be true before this step",\n            "postconditions": "what will be true after this step"\n        }},\n        ...\n    ],\n    "clarifications_needed": [],\n    "safety_constraints": []\n}}\n\nIf the command is ambiguous, add clarification questions.\nAlways include safety checks.\n"""\n\n        try:\n            response = requests.post(\n                self.ollama_url,\n                json={\n                    "model": self.model,\n                    "prompt": prompt,\n                    "stream": False,\n                    "temperature": 0.3  # Deterministic for planning\n                },\n                timeout=30\n            )\n\n            output = response.json()["response"]\n\n            # Extract JSON from response\n            import re\n            json_match = re.search(r\'\\{.*\\}\', output, re.DOTALL)\n            if json_match:\n                plan = json.loads(json_match.group())\n                return plan\n\n        except Exception as e:\n            self.get_logger().error(f"LLM error: {e}")\n\n        return {"error": "Failed to generate plan"}\n\ndef main(args=None):\n    rclpy.init(args=args)\n    planner = TaskPlannerNode()\n    rclpy.spin(planner)\n    planner.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(e.h2,{id:"example-plan-output",children:"Example Plan Output"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Command"}),': "Pick up the red cube from the table and place it on the shelf"']}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Generated Plan"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-json",children:'{\n    "task": "Place red cube on shelf",\n    "steps": [\n        {\n            "step_number": 1,\n            "description": "Navigate to table",\n            "ros_action": "navigate",\n            "parameters": {\n                "target_location": "table_front",\n                "max_speed": 0.3\n            },\n            "preconditions": "Robot localized, navigation available",\n            "postconditions": "Robot near table, ready to perceive object"\n        },\n        {\n            "step_number": 2,\n            "description": "Look for red cube using camera",\n            "ros_action": "perception",\n            "parameters": {\n                "object_to_find": "red cube",\n                "color": "red",\n                "method": "yolo_detection"\n            },\n            "preconditions": "Camera operational, scene illuminated",\n            "postconditions": "Cube location known in 3D space"\n        },\n        {\n            "step_number": 3,\n            "description": "Approach cube for grasping",\n            "ros_action": "move_arm",\n            "parameters": {\n                "target_pose_type": "approach",\n                "distance_from_object": 0.05\n            },\n            "preconditions": "Cube detected, no obstacles",\n            "postconditions": "Gripper 5cm from cube"\n        },\n        {\n            "step_number": 4,\n            "description": "Grasp cube with controlled force",\n            "ros_action": "grasp",\n            "parameters": {\n                "gripper_force": 50,\n                "force_unit": "newtons"\n            },\n            "preconditions": "Gripper in grasp position, no obstruction",\n            "postconditions": "Cube grasped, force feedback received"\n        },\n        {\n            "step_number": 5,\n            "description": "Lift cube to safe height",\n            "ros_action": "move_arm",\n            "parameters": {\n                "direction": "up",\n                "distance": 0.2\n            },\n            "preconditions": "Cube grasped, arm ready",\n            "postconditions": "Cube elevated, clear of table"\n        },\n        {\n            "step_number": 6,\n            "description": "Navigate to shelf location",\n            "ros_action": "navigate",\n            "parameters": {\n                "target_location": "shelf_base"\n            },\n            "preconditions": "Cube lifted, balance maintained",\n            "postconditions": "Robot positioned at shelf"\n        },\n        {\n            "step_number": 7,\n            "description": "Place cube on shelf",\n            "ros_action": "release",\n            "parameters": {\n                "target_height": "shelf_top",\n                "release_force": 10\n            },\n            "preconditions": "Gripper above shelf, no obstacles",\n            "postconditions": "Cube on shelf, gripper open"\n        }\n    ],\n    "clarifications_needed": [],\n    "safety_constraints": [\n        "Maintain balance during cube manipulation",\n        "Check for obstacles in path to shelf",\n        "Verify gripper force is within limits",\n        "Confirm shelf can support cube weight"\n    ]\n}\n'})}),"\n",(0,s.jsx)(e.h2,{id:"handling-ambiguity",children:"Handling Ambiguity"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'class DisambiguationEngine(Node):\n    def ask_clarification(self, clarifications: list) -> dict:\n        """Ask user for clarification when command is ambiguous."""\n\n        if not clarifications:\n            return {}\n\n        responses = {}\n\n        for clarification in clarifications:\n            self.get_logger().info(f"Clarification needed: {clarification}")\n\n            # In real system, would wait for user input\n            # For now, use defaults\n            if "location" in clarification.lower():\n                responses["location"] = "default_location"\n            elif "object" in clarification.lower():\n                responses["object"] = "largest_available_object"\n\n        return responses\n'})}),"\n",(0,s.jsx)(e.h2,{id:"safety-aware-planning",children:"Safety-Aware Planning"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"class SafetyAwarePlanner:\n    def validate_plan(self, plan: dict) -> bool:\n        \"\"\"Validate plan for safety violations.\"\"\"\n\n        unsafe_keywords = ['destroy', 'break', 'hurt', 'harm', 'dangerous']\n\n        for step in plan.get('steps', []):\n            description = step.get('description', '').lower()\n            for keyword in unsafe_keywords:\n                if keyword in description:\n                    return False\n\n        # Check preconditions\n        for step in plan.get('steps', []):\n            preconditions = step.get('preconditions', '')\n            if not self.can_satisfy_preconditions(preconditions):\n                return False\n\n        return True\n\n    def can_satisfy_preconditions(self, preconditions: str) -> bool:\n        \"\"\"Check if preconditions can be satisfied.\"\"\"\n        # In real system, query robot state\n        # For demo, assume yes\n        return True\n"})}),"\n",(0,s.jsx)(e.h2,{id:"next-lesson",children:"Next Lesson"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.a,{href:"/docs/modules/m4-vla/m4-capstone-project",children:"Lesson 3: Capstone Project"})})]})}function d(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(p,{...n})}):p(n)}},8453:(n,e,a)=>{a.d(e,{R:()=>i,x:()=>r});var t=a(6540);const s={},o=t.createContext(s);function i(n){const e=t.useContext(o);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:i(n.components),t.createElement(o.Provider,{value:e},n.children)}}}]);